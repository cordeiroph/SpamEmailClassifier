{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of it is to generate a model capable to classify a email as spam or not spam.\n",
    "\n",
    "The dataset used was from http://www2.aueb.gr/users/ion/data/enron-spam/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/phrc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/phrc/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "import email\n",
    "from email.message import EmailMessage\n",
    "from email.parser import BytesParser, Parser\n",
    "from email.policy import default\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDf(path):\n",
    "    \"\"\"\n",
    "    Read all email files and convert to a dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        Path to a root directory to be read\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    dataframe\n",
    "        a dataframe with x columns based in all emails properties\n",
    "    \"\"\"\n",
    "    os.chdir(path)\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "         for file in files:\n",
    "            with open(os.path.join(root, file), \"rb\") as openFile:\n",
    "                dictTemp = {}\n",
    "                try:\n",
    "                    message = email.message_from_binary_file(openFile, policy=default)\n",
    "                    #Parser(policy=default).parsestr(openFile.read())\n",
    "                    dictTemp['file'] = openFile.name\n",
    "                    for key in message.keys():\n",
    "                        dictTemp[key.lower()] = message[key]\n",
    "                    dictTemp['messageType'] = message.get_content_type()\n",
    "                    body = message.get_body()\n",
    "                    if body['content-type'].maintype == 'text':\n",
    "                        if body['content-type'].subtype == 'plain':\n",
    "                            dictTemp['messageStr'] = str(body.get_content())\n",
    "                        elif body['content-type'].subtype == 'html':\n",
    "                            dictTemp['messageStr'] = str(body)\n",
    "                    elif body['content-type'].content_type in 'multipart':\n",
    "                        dictTemp['messageStr'] = str(body.get_body(preferencelist=('html')))\n",
    "                    dictTemp['parseError'] = False                 \n",
    "                except:\n",
    "                    dictTemp['parseError'] = True\n",
    "                data.append(dictTemp)\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def emailTextCleanner(text):\n",
    "    \"\"\"\n",
    "    Remove:\n",
    "        Html Tags\n",
    "        Email headers\n",
    "        Ponctuation\n",
    "        break lines and tabs\n",
    "    \n",
    "    And convert the string to lower case \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    str\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = str(text).lower()\n",
    "        clean = re.compile('charset.*\\n')\n",
    "        text = re.sub(clean, '', text)\n",
    "        clean = re.compile('content-.*\\n')\n",
    "        text = re.sub(clean, '', text)\n",
    "        clean = re.compile('received: from.*\\n')\n",
    "        text = re.sub(clean, '', text)\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = text.replace('\\t', ' ')\n",
    "        clean = re.compile('<.*?>')\n",
    "        text = re.sub(clean, '', str(text))\n",
    "        clean = re.compile('['+string.punctuation+']')\n",
    "        text = re.sub(clean, '', text)\n",
    "        \n",
    "        return text\n",
    "    except:\n",
    "        print(type(text))\n",
    "        return \"\"\n",
    "#    text = text.replace('Subject:', '', 1)\n",
    "    #print(text)\n",
    "    #print(\"---------------------------------------------\")\n",
    "    #print(\"---------------------------------------------\")\n",
    "    #text = TextBlob(text)\n",
    "    #text = str(text.correct())\n",
    "    #print(text)\n",
    "    #print(\"\\n\")\n",
    "#    return text\n",
    "\n",
    "def lemmatizeList(words):\n",
    "    \"\"\"\n",
    "    Lemmatize a list of string\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    words: list of str\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    list of str\n",
    "    \"\"\"\n",
    "#    print(\"|\", end='')\n",
    "    lem = WordNetLemmatizer()\n",
    "    for i in range(len(words)):\n",
    "#        text = TextBlob(words[i])\n",
    "#        words[i] = str(text.correct())\n",
    "        words[i] = lem.lemmatize(words[i], 'v')\n",
    "        words[i] = lem.lemmatize(words[i], 'n')\n",
    "    return words\n",
    "\n",
    "def revomeWordsWithOneCharacter(words):\n",
    "    return list(filter(lambda x : len(x) > 1, words))\n",
    "\n",
    "def removeDigits(words):\n",
    "    return list(filter(lambda x : x.isdigit() == False, words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spamPath = '/Users/phrc/Documents/Projects/pythonProject/SpamEmailClassifier/emails/spam/'\n",
    "hamPath = '/Users/phrc/Documents/Projects/pythonProject/SpamEmailClassifier/emails/ham/'\n",
    "\n",
    "\n",
    "dfSpam = createDf(spamPath)\n",
    "dfHam = createDf(hamPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.bar(np.arange(2), (len(dfHam), len(dfSpam)), align='center')\n",
    "plt.xticks(np.arange(2), ('Ham \\n{}'.format(len(dfHam)), 'Spam \\n{}'.format(len(dfSpam))))\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSpam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Spam Columns Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSpamDesc = dfSpam.describe(include=['object']).T\n",
    "n = len(dfSpam) * 0.75 \n",
    "dfSpamDesc[dfSpamDesc['count'] > n].head(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Ham Columns Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHamDesc = dfHam.describe(include=['object']).T\n",
    "n = len(dfHam) * 0.75 \n",
    "dfHamDesc[dfHamDesc['count']> 15000].head(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing unnescessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSpam = dfSpam[['content-type', 'date', 'from', 'messageStr', 'messageType', 'subject', 'to', 'parseError', 'file']]\n",
    "dfHam = dfHam[['content-type', 'date', 'from', 'messageStr', 'messageType', 'subject', 'to', 'parseError', 'file']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emails with Parser Error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHamP = dfHam[dfHam['parseError'] == True]\n",
    "dfSpamP = dfSpam[dfSpam['parseError'] == True]\n",
    "hamFreq = (len(dfHamP) * 100 / len(dfHam))\n",
    "spamFreq = (len(dfSpamP) * 100 / len(dfSpam))\n",
    "plt.bar(np.arange(2), (hamFreq, spamFreq), align='center')\n",
    "plt.xticks(np.arange(2), ('Ham \\n{0:.2}%'.format(hamFreq), 'Spam \\n{0:.2}%'.format(spamFreq)))\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing emails with parser problems "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSpam = dfSpam[dfSpam['parseError'] == False]\n",
    "dfHam = dfHam[dfHam['parseError'] == False]\n",
    "\n",
    "del dfHam['parseError']\n",
    "del dfSpam['parseError']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSpam['isSpam'] = True\n",
    "dfHam['isSpam'] = False\n",
    "\n",
    "print(dfHam.columns.values)\n",
    "print(dfSpam.columns.values)\n",
    "\n",
    "dfMaster = pd.concat([dfSpam, dfHam])\n",
    "print(len(dfMaster))\n",
    "dfMaster.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check email message and email type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (16,12))\n",
    "grid = plt.GridSpec(2,4, wspace =0.3, hspace =0.5)\n",
    "barAll = fig.add_subplot(grid[0,0:3])\n",
    "barNonNull = fig.add_subplot(grid[1,0:3])\n",
    "barNull = fig.add_subplot(grid[0:2,3])\n",
    "\n",
    "barAll.bar(\n",
    "    dfMaster['messageType'].unique(), \n",
    "    dfMaster['messageType'].value_counts(), \n",
    "    align='center'\n",
    ")\n",
    "barAll.set_title('All emails')\n",
    "\n",
    "barNonNull.bar(\n",
    "    dfMaster[dfMaster['messageStr'].notnull()]['messageType'].unique(), \n",
    "    dfMaster[dfMaster['messageStr'].notnull()]['messageType'].value_counts(), \n",
    "    align='center'\n",
    ")\n",
    "\n",
    "barNonNull.set_title('Non null meassages')\n",
    "\n",
    "barNull.bar(\n",
    "    dfMaster[dfMaster['messageStr'].isnull()]['messageType'].unique(), \n",
    "    dfMaster[dfMaster['messageStr'].isnull()]['messageType'].value_counts(), \n",
    "    align='center'\n",
    ")\n",
    "\n",
    "barNull.set_title('Null meassages')\n",
    "\n",
    "fig.text(0.5, 0.04, 'Email Type', ha='center', fontsize=15)\n",
    "fig.text(0.04, 0.5, 'Emails', va='center', rotation='vertical', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null messages proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie([len(dfMaster[dfMaster['messageStr'].notnull()]), len(dfMaster[dfMaster['messageStr'].isnull()])], \n",
    "        labels=['Non Null', 'Null'], autopct='%1.0f%%', pctdistance=0.5, labeldistance=1.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove null messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMaster = dfMaster[dfMaster['messageStr'].notnull()]\n",
    "print(len(dfMaster[dfMaster['messageStr'].isnull()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove HTML tags from the messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMaster['treatedMessage'] = dfMaster['messageStr'].apply(emailTextCleanner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMaster.describe(include=['object']).T.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTemp = dfMaster[dfMaster['messageType'].str.contains('multipart')]\n",
    "len(dfTemp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTemp.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Stop Words\n",
    "stop = text.ENGLISH_STOP_WORDS\n",
    "pat = r'\\b(?:{})\\b'.format('|'.join(stop))\n",
    "dfMaster['treatedMessage'] = dfMaster['treatedMessage'].str.replace(pat, '')\n",
    "\n",
    "#Create a column with list of words\n",
    "dfMaster['wordsList'] = dfMaster['treatedMessage'].str.split().apply(lemmatizeList).apply(revomeWordsWithOneCharacter).apply(removeDigits)\n",
    "\n",
    "\n",
    "\n",
    "#Create a columns to calculate the total amount of words\n",
    "dfMaster['totalTreatedWords'] = dfMaster['wordsList'].apply(lambda x : len(x))\n",
    "dfMaster['treatedTextLen'] = dfMaster['treatedMessage'].apply(lambda x : len(x))\n",
    "dfMaster['textLen'] = dfMaster['messageStr'].apply(lambda x : len(str(x)))\n",
    "dfMaster['uniqueWordsLen'] = dfMaster['wordsList'].apply(lambda x : len(set(x)))\n",
    "\n",
    "dfMaster.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset distribution by spam type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfMaster\n",
    "\n",
    "print(df['isSpam'].value_counts().rename({False: 'Ham', True: 'Spam'}))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (18,6))\n",
    "grid = plt.GridSpec(1,2, wspace =0.1)\n",
    "barPlt = fig.add_subplot(grid[0,0])\n",
    "piePlt = fig.add_subplot(grid[0,1])\n",
    "\n",
    "\n",
    "barPlt.bar(\n",
    "    ('Spam', 'Ham'), \n",
    "    [len(df[df['isSpam'] == True]), len(df[df['isSpam'] == False])], \n",
    "    align='center')\n",
    "barPlt.set_xticks(np.arange(2), ['Spam', 'Ham'])\n",
    "\n",
    "\n",
    "piePlt.pie(df['isSpam'].value_counts(), labels=['Spam', 'Ham'], autopct='%1.0f%%', pctdistance=0.5, labeldistance=1.2)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare text lenght for ham and spam  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 4 differents attributes what define the text lengh:\n",
    "\n",
    "- textLen : This is the raw text without any treatment\n",
    "- treatedTextLen: This is the text after removing the stop words and ponctuation\n",
    "- totalTreatedWords: This is the amount of words used in the text after the treatment, basically this exclude space and line breaks\n",
    "- uniqueWordsLen: This is the amount of unique words used in the treated text, basically it removes repited words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Text size lengh comparison  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (18,6))\n",
    "grid = plt.GridSpec(1,4, wspace =0.8)\n",
    "histPlt = fig.add_subplot(grid[0,0:2])\n",
    "boxPlt1 = fig.add_subplot(grid[0,2])\n",
    "boxPlt2 = fig.add_subplot(grid[0,3])\n",
    "\n",
    "histPlt.hist(\n",
    "    [\n",
    "        df[df['isSpam'] == True]['textLen'], \n",
    "        df[df['isSpam'] == False]['textLen']\n",
    "    ], \n",
    "    np.linspace(0, df['textLen'].quantile(0.75), 30), \n",
    "    density = True, \n",
    "    label=['Spam', 'Ham']\n",
    ") \n",
    "histPlt.legend(loc='upper right')\n",
    "\n",
    "boxPlt1.boxplot(\n",
    "    (\n",
    "         df[df['isSpam'] == True]['textLen'], \n",
    "         df[df['isSpam'] == False]['textLen'] \n",
    "    ), \n",
    "    labels = ('Spam', 'Ham')\n",
    ")\n",
    "\n",
    "boxPlt2.boxplot(\n",
    "    (\n",
    "        df[(df['isSpam'] == True) & (df['textLen'] < df['textLen'].quantile(0.75))]['textLen'], \n",
    "        df[(df['isSpam'] == False) & (df['textLen'] < df['textLen'].quantile(0.75))]['textLen'] \n",
    "    ), \n",
    "    labels = ('Spam', 'Ham')\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text size without stop words and punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (18,6))\n",
    "grid = plt.GridSpec(1,4, wspace =0.8)\n",
    "histPlt = fig.add_subplot(grid[0,0:2])\n",
    "boxPlt1 = fig.add_subplot(grid[0,2])\n",
    "boxPlt2 = fig.add_subplot(grid[0,3])\n",
    "\n",
    "histPlt.hist(\n",
    "    [\n",
    "        df[df['isSpam'] == True]['treatedTextLen'], \n",
    "        df[df['isSpam'] == False]['treatedTextLen']\n",
    "    ],  \n",
    "    np.linspace(0, df['treatedTextLen'].quantile(0.75), 30),\n",
    "    density= True,\n",
    "    label=['Spam', 'Ham']\n",
    ") \n",
    "histPlt.legend(loc='upper right')\n",
    "\n",
    "boxPlt1.boxplot(\n",
    "    (\n",
    "        df[df['isSpam'] == True]['treatedTextLen'], \n",
    "        df[df['isSpam'] == False]['treatedTextLen'] ), \n",
    "    labels = ('Spam', 'Ham')\n",
    ")\n",
    "\n",
    "boxPlt2.boxplot(\n",
    "    (\n",
    "        df[(df['isSpam'] == True) & (df['treatedTextLen'] < df['treatedTextLen'].quantile(0.75))]['treatedTextLen'], \n",
    "        df[(df['isSpam'] == False) & (df['treatedTextLen'] < df['treatedTextLen'].quantile(0.75))]['treatedTextLen'] \n",
    "    ), \n",
    "    labels = ('Spam', 'Ham')\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total words used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (18,6))\n",
    "grid = plt.GridSpec(1,4, wspace =0.8)\n",
    "histPlt = fig.add_subplot(grid[0,0:2])\n",
    "boxPlt1 = fig.add_subplot(grid[0,2])\n",
    "boxPlt2 = fig.add_subplot(grid[0,3])\n",
    "\n",
    "histPlt.hist(\n",
    "    [\n",
    "        df[df['isSpam'] == True]['totalTreatedWords'], \n",
    "        df[df['isSpam'] == False]['totalTreatedWords']\n",
    "    ],  \n",
    "    np.linspace(0, df['totalTreatedWords'].quantile(0.75), 30),\n",
    "    density= True,\n",
    "    label=['Spam', 'Ham']\n",
    ") \n",
    "histPlt.legend(loc='upper right')\n",
    "\n",
    "boxPlt1.boxplot(\n",
    "    (\n",
    "        df[df['isSpam'] == True]['totalTreatedWords'], \n",
    "        df[df['isSpam'] == False]['totalTreatedWords'] \n",
    "    ), \n",
    "    labels = ('Spam', 'Ham')\n",
    ")\n",
    "\n",
    "boxPlt2.boxplot(\n",
    "    (\n",
    "        df[(df['isSpam'] == True) & (df['totalTreatedWords'] < df['totalTreatedWords'].quantile(0.75))]['totalTreatedWords'], \n",
    "        df[(df['isSpam'] == False) & (df['totalTreatedWords'] < df['totalTreatedWords'].quantile(0.75))]['totalTreatedWords'] \n",
    "    ), \n",
    "    labels = ('Spam', 'Ham')\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total unique words used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (18,6))\n",
    "grid = plt.GridSpec(1,4, wspace =0.8)\n",
    "histPlt = fig.add_subplot(grid[0,0:2])\n",
    "boxPlt1 = fig.add_subplot(grid[0,2])\n",
    "boxPlt2 = fig.add_subplot(grid[0,3])\n",
    "\n",
    "histPlt.hist(\n",
    "    [\n",
    "        df[df['isSpam'] == True]['uniqueWordsLen'], \n",
    "        df[df['isSpam'] == False]['uniqueWordsLen']\n",
    "    ],  \n",
    "    np.linspace(0, df['uniqueWordsLen'].quantile(0.75), 30),\n",
    "    density= True,\n",
    "    label=['Spam', 'Ham']\n",
    ") \n",
    "histPlt.legend(loc='upper right')\n",
    "\n",
    "boxPlt1.boxplot(\n",
    "    (\n",
    "        df[df['isSpam'] == True]['uniqueWordsLen'], \n",
    "        df[df['isSpam'] == False]['uniqueWordsLen'] \n",
    "    ), \n",
    "    labels = ('Spam', 'Ham')\n",
    ")\n",
    "\n",
    "boxPlt2.boxplot(\n",
    "    (\n",
    "        df[(df['isSpam'] == True) & (df['uniqueWordsLen'] < df['uniqueWordsLen'].quantile(0.75))]['uniqueWordsLen'], \n",
    "        df[(df['isSpam'] == False) & (df['uniqueWordsLen'] < df['uniqueWordsLen'].quantile(0.75))]['uniqueWordsLen'] \n",
    "    ), \n",
    "    labels = ('Spam', 'Ham')\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "# Need to do \n",
    "\n",
    "----It's not clear the text size can influence into the classification of the email in spam or ham.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total uniques words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter(list(chain(*df[\"wordsList\"].values)))\n",
    "\n",
    "countSpam = Counter(list(chain(*df[df['isSpam'] == True][\"wordsList\"].values)))\n",
    "\n",
    "countHam = Counter(list(chain(*df[df['isSpam'] == False][\"wordsList\"].values)))\n",
    "\n",
    "fig = plt.figure(figsize = (18,6))\n",
    "grid = plt.GridSpec(1,2, wspace =0.2)\n",
    "pltBar = fig.add_subplot(grid[0,0])\n",
    "pltText = fig.add_subplot(grid[0,1])\n",
    "\n",
    "\n",
    "pltBar.bar(\n",
    "    ['Total unique words', 'Spam unique words', 'Ham unique words'], \n",
    "    [len(count), len(countSpam), len(countHam)], \n",
    "    align='center'\n",
    ")\n",
    "\n",
    "textWords = ['Total unique words:              {}'.format(len(count)), \n",
    "             'Total spam unique words:    {}'.format(len(countSpam)), \n",
    "             'Total ham unique words:      {}'.format(len(countHam))]\n",
    "\n",
    "\n",
    "pltText.text(x=0, y=0.5, s = '\\n'.join(textWords), fontsize = 18) \n",
    "pltText.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countSpam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20 Most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWords = pd.DataFrame(list(dict(count).items()))\n",
    "dfWords.columns = ['word', 'occur'] \n",
    "totalOcurr = dfWords['occur'].sum()\n",
    "dfWords['freq'] = dfWords['occur'] / totalOcurr  \n",
    "dfWords = dfWords.sort_values(by='freq', ascending=False)\n",
    "dfWords = dfWords.reset_index(drop=True)\n",
    "dfWords['freqAcum'] = dfWords['freq'].cumsum()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (18,12))\n",
    "grid = plt.GridSpec(4,2, wspace =0.2, hspace = 0.5)\n",
    "pltLine = fig.add_subplot(grid[0:2,0])\n",
    "pltBar = fig.add_subplot(grid[2:4,0:2])\n",
    "pltTable = fig.add_subplot(grid[0:2,1])\n",
    "\n",
    "pltLine.plot(dfWords['freqAcum'], range(len(dfWords)))\n",
    "pltLine.set_ylabel('Number of Words')\n",
    "pltLine.set_xlabel('Acumulative Frequence')\n",
    "pltLine.set_title('Line Graph of acumulative frequence')\n",
    "pltLine.grid(True)\n",
    "\n",
    "\n",
    "dfTWord = dfWords.loc[[0,20,50,100,500,1000,5000,10000,15000]][['freqAcum']]\n",
    "\n",
    "pltTable.table(cellText=dfTWord.values, rowLabels= dfTWord.index, colLabels = dfTWord.columns, loc='best')\n",
    "pltTable.axis('off')\n",
    "pltTable.set_title('Table of acumulative frequence')\n",
    "\n",
    "\n",
    "\n",
    "pltBar.bar(dfWords.head(20)['word'], dfWords.head(20)['freq'])\n",
    "pltBar.set_title('20 Most Used word')\n",
    "\n",
    "yPos = np.arange(20)\n",
    "pltBar.set_xticklabels(dfWords.head(20)[\"word\"], rotation=60)\n",
    "pltBar.set_xticks(yPos)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20 Most common words on Spam emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSpamWords = pd.DataFrame(list(dict(countSpam).items()))\n",
    "dfSpamWords.columns = ['word', 'occur'] \n",
    "totalSpamOcurr = dfSpamWords['occur'].sum()\n",
    "dfSpamWords['freq'] = dfSpamWords['occur'] / totalSpamOcurr  \n",
    "dfSpamWords = dfSpamWords.sort_values(by='freq', ascending=False)\n",
    "dfSpamWords = dfSpamWords.reset_index(drop=True)\n",
    "dfSpamWords['freqAcum'] = dfSpamWords['freq'].cumsum()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (18,12))\n",
    "grid = plt.GridSpec(4,2, wspace =0.2, hspace = 0.5)\n",
    "pltLine = fig.add_subplot(grid[0:2,0])\n",
    "pltBar = fig.add_subplot(grid[2:4,0:2])\n",
    "pltTable = fig.add_subplot(grid[0:2,1])\n",
    "\n",
    "pltLine.plot(dfSpamWords['freqAcum'], range(len(dfSpamWords)))\n",
    "pltLine.set_ylabel('Number of Words')\n",
    "pltLine.set_xlabel('Acumulative Frequence')\n",
    "pltLine.set_title('Line Graph of acumulative frequence')\n",
    "pltLine.grid(True)\n",
    "\n",
    "\n",
    "dfSWord = dfSpamWords.loc[[0,20,50,100,500,1000,5000,10000,15000]][['freqAcum']]\n",
    "\n",
    "pltTable.table(cellText=dfSWord.values, rowLabels= dfSWord.index, colLabels = dfSWord.columns, loc='best')\n",
    "pltTable.axis('off')\n",
    "pltTable.set_title('Table of acumulative frequence')\n",
    "\n",
    "\n",
    "\n",
    "pltBar.bar(dfSpamWords.head(20)['word'], dfSpamWords.head(20)['freq'], align='center')\n",
    "pltBar.set_title('20 Most Used word')\n",
    "\n",
    "yPos = np.arange(20)\n",
    "pltBar.set_xticklabels(dfSpamWords.head(20)[\"word\"], rotation=60)\n",
    "pltBar.set_xticks(yPos)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20 Most common words on Ham emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHamWords = pd.DataFrame(list(dict(countHam).items()))\n",
    "dfHamWords.columns = ['word', 'occur'] \n",
    "totalHamOcurr = dfHamWords['occur'].sum()\n",
    "dfHamWords['freq'] = dfHamWords['occur'] / totalHamOcurr  \n",
    "dfHamWords = dfHamWords.sort_values(by='freq', ascending=False)\n",
    "dfHamWords = dfHamWords.reset_index(drop=True)\n",
    "dfHamWords['freqAcum'] = dfHamWords['freq'].cumsum()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (18,12))\n",
    "grid = plt.GridSpec(4,2, wspace =0.2, hspace = 0.5)\n",
    "pltLine = fig.add_subplot(grid[0:2,0])\n",
    "pltBar = fig.add_subplot(grid[2:4,0:2])\n",
    "pltTable = fig.add_subplot(grid[0:2,1])\n",
    "\n",
    "pltLine.plot(dfHamWords['freqAcum'], range(len(dfHamWords)))\n",
    "pltLine.set_ylabel('Number of Words')\n",
    "pltLine.set_xlabel('Acumulative Frequence')\n",
    "pltLine.set_title('Line Graph of acumulative frequence')\n",
    "pltLine.grid(True)\n",
    "\n",
    "\n",
    "dfHWord = dfHamWords.loc[[0,20,50,100,500,1000,5000,10000,15000]][['freqAcum']]\n",
    "\n",
    "pltTable.table(cellText=dfHWord.values, rowLabels= dfHWord.index, colLabels = dfHWord.columns, loc='best')\n",
    "pltTable.axis('off')\n",
    "pltTable.set_title('Table of acumulative frequence')\n",
    "\n",
    "\n",
    "\n",
    "pltBar.bar(dfHamWords.head(20)['word'], dfHamWords.head(20)['freq'], align='center')\n",
    "pltBar.set_title('20 Most Used word')\n",
    "\n",
    "yPos = np.arange(20)\n",
    "pltBar.set_xticklabels(dfHamWords.head(20)[\"word\"], rotation=60)\n",
    "pltBar.set_xticks(yPos)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempSet = sorted(set(list(chain(*df[df['isSpam'] == True][\"wordsList\"].values))))\n",
    "\n",
    "\n",
    "print(tempSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "# find those words that may be misspelled\n",
    "misspelled = spell.unknown(tempSet)\n",
    "\n",
    "print(misspelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "words = lem.lemmatize('accommodations')\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting prepare data set for modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(chain(*df[\"wordsList\"].values))\n",
    "\n",
    "l = set(filter(lambda k : len(k) == 1, l))\n",
    "\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWordsG = pd.DataFrame(list(dict(count).items()))\n",
    "dfWordsG.columns = ['word', 'occur'] \n",
    "totalOcurrG = dfWordsG['occur'].sum()\n",
    "dfWordsG['freq'] = dfWordsG['occur'] / totalOcurrG  \n",
    "dfWordsG = dfWordsG.sort_values(by='freq', ascending=False)\n",
    "dfWordsG = dfWordsG.reset_index(drop=True)\n",
    "dfWordsG['freqAcum'] = dfWordsG['freq'].cumsum()\n",
    "\n",
    "dfSpamWordsG = pd.DataFrame(list(dict(countSpam).items()))\n",
    "dfSpamWordsG.columns = ['word', 'occur'] \n",
    "totalSpamOcurrG = dfSpamWordsG['occur'].sum()\n",
    "dfSpamWordsG['freq'] = dfSpamWordsG['occur'] / totalOcurrG  \n",
    "dfSpamWordsG = dfSpamWordsG.sort_values(by='freq', ascending=False)\n",
    "dfSpamWordsG = dfSpamWordsG.reset_index(drop=True)\n",
    "dfSpamWordsG['freqAcum'] = dfSpamWordsG['freq'].cumsum()\n",
    "\n",
    "dfHamWordsG = pd.DataFrame(list(dict(countHam).items()))\n",
    "dfHamWordsG.columns = ['word', 'occur'] \n",
    "totalHamOcurrG = dfHamWordsG['occur'].sum()\n",
    "dfHamWordsG['freq'] = dfHamWordsG['occur'] / totalOcurrG  \n",
    "dfHamWordsG = dfHamWordsG.sort_values(by='freq', ascending=False)\n",
    "dfHamWordsG = dfHamWordsG.reset_index(drop=True)\n",
    "dfHamWordsG['freqAcum'] = dfHamWordsG['freq'].cumsum()\n",
    "\n",
    "fig = plt.figure(figsize = (18,6))\n",
    "grid = plt.GridSpec(1,3, wspace =0.2)\n",
    "pltTotal = fig.add_subplot(grid[0,0])\n",
    "pltSpam = fig.add_subplot(grid[0,1])\n",
    "pltHam = fig.add_subplot(grid[0,2])\n",
    "\n",
    "pltTotal.plot(dfWords['freqAcum'], range(len(dfWordsG)))\n",
    "pltTotal.set_ylabel('Total Words')\n",
    "pltTotal.set_xlabel('Acumulative Frequence')\n",
    "pltTotal.set_title('All Words')\n",
    "pltTotal.grid(True)\n",
    "\n",
    "pltSpam.plot(dfSpamWords['freqAcum'], range(len(dfSpamWordsG)))\n",
    "pltSpam.set_ylabel('Total Spam Words')\n",
    "pltSpam.set_xlabel('Acumulative Frequence')\n",
    "pltSpam.set_title('Spam Words')\n",
    "pltSpam.grid(True)\n",
    "\n",
    "pltHam.plot(dfHamWords['freqAcum'], range(len(dfHamWordsG)))\n",
    "pltHam.set_ylabel('Total Ham Words')\n",
    "pltHam.set_xlabel('Acumulative Frequence')\n",
    "pltHam.set_title('Ham Words')\n",
    "pltHam.grid(True)\n",
    "\n",
    "\n",
    "fig2 = plt.figure(figsize = (18,6))\n",
    "grid2 = plt.GridSpec(1,6, wspace =0.4)\n",
    "\n",
    "pltTotalTableG = fig2.add_subplot(grid2[0,0:2])\n",
    "pltSpamTableG = fig2.add_subplot(grid2[0,2:4])\n",
    "pltHamTableG = fig2.add_subplot(grid2[0,4:6])\n",
    "\n",
    "dfTWordG = dfWordsG.loc[[0,50,100,500,1000,5000,10000,15000]][['freqAcum']]\n",
    "\n",
    "pltTotalTableG.table(cellText=dfTWordG.values, rowLabels= dfTWordG.index, colLabels = dfTWordG.columns, loc='best')\n",
    "pltTotalTableG.axis('off')\n",
    "\n",
    "dfSWordG = dfSpamWordsG.loc[[0,50,100,500,1000,5000,10000,15000]][['freqAcum']]\n",
    "\n",
    "pltSpamTableG.table(cellText=dfSWordG.values, rowLabels= dfSWordG.index, colLabels = dfSWordG.columns, loc='best')\n",
    "pltSpamTableG.axis('off')\n",
    "\n",
    "dfHWordG = dfHamWordsG.loc[[0,50,100,500,1000,5000,10000,15000]][['freqAcum']]\n",
    "\n",
    "pltHamTableG.table(cellText=dfHWordG.values, rowLabels= dfHWordG.index, colLabels = dfHWordG.columns, loc='best')\n",
    "pltHamTableG.axis('off')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
