{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of it is to generate a model capable to classify a email as spam or not spam.\n",
    "\n",
    "The dataset used was from http://www2.aueb.gr/users/ion/data/enron-spam/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/phrc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/phrc/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from email.message import EmailMessage\n",
    "from email.parser import BytesParser, Parser\n",
    "from email.policy import default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDf(path, isSpam):\n",
    "    \"\"\"\n",
    "    Insert all the email context into a dataframe column and classify if the email is a spam or not\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        Path o directory to be read\n",
    "    isSpam: Boolean\n",
    "        true is the files are spam or false otherwise        \n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    dataframe\n",
    "        a dataframe with 2 columns isSpam and emailContext\n",
    "    \"\"\"\n",
    "    os.chdir(path)\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "         for file in files:\n",
    "            with open(os.path.join(root, file), \"r\") as openFile:\n",
    "                dictTemp = {}\n",
    "                try:\n",
    "                    message = Parser(policy=default).parsestr(openFile.read())\n",
    "                    dictTemp['file'] = openFile.name\n",
    "                    for key in message.keys():\n",
    "                        dictTemp[key.lower()] = message[key]\n",
    "                    dictTemp['messageType'] = message.get_content_type()\n",
    "                    if message.get_content_type() == 'text/plain':\n",
    "                        try:\n",
    "                            dictTemp['message'] = str(message.get_content())\n",
    "                        except:\n",
    "                            dictTemp['message'] = str(message.get_body())\n",
    "                            dictTemp['messageType'] = 'Error'\n",
    "                    else:\n",
    "                        dictTemp['message'] = str(message.get_body())\n",
    "                    dictTemp['parseError'] = False                    \n",
    "                except:\n",
    "                    dictTemp['parseError'] = True\n",
    "                data.append(dictTemp)\n",
    "    df = pd.DataFrame(data)\n",
    "    df['isSpam'] = isSpam\n",
    "    \n",
    "#print(headers.keys())\n",
    "\n",
    "#print(headers.values())\n",
    "#str(headers.get_content_type())\n",
    "#str(headers.get_body())\n",
    "#str(headers.get_content())\n",
    "\n",
    "    \n",
    "#df.columns = ['text', 'isSpam', 't1', 't2']\n",
    "    return df\n",
    "\n",
    "def emailTextCleanner(text):\n",
    "    \"\"\"\n",
    "    Remove the first apperance of the word 'Subject:'\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    str\n",
    "    \"\"\"\n",
    "    text = text.replace('Subject:', '', 1)\n",
    "    #print(text)\n",
    "    #print(\"---------------------------------------------\")\n",
    "    #print(\"---------------------------------------------\")\n",
    "    #text = TextBlob(text)\n",
    "    #text = str(text.correct())\n",
    "    #print(text)\n",
    "    #print(\"\\n\")\n",
    "    return text\n",
    "\n",
    "def lemmatizeList(words):\n",
    "    \"\"\"\n",
    "    Lemmatize a list of string\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    words: list of str\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    list of str\n",
    "    \"\"\"\n",
    "#    print(\"|\", end='')\n",
    "    lem = WordNetLemmatizer()\n",
    "    for i in range(len(words)):\n",
    "#        text = TextBlob(words[i])\n",
    "#        words[i] = str(text.correct())\n",
    "        words[i] = lem.lemmatize(words[i], 'v')\n",
    "        words[i] = lem.lemmatize(words[i], 'n')\n",
    "    return words\n",
    "\n",
    "def revomeWordsWithOneCharacter(words):\n",
    "    return list(filter(lambda x : len(x) > 1, words))\n",
    "\n",
    "def removeDigits(words):\n",
    "    return list(filter(lambda x : x.isdigit() == False, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0x-accept-language</th>\n",
       "      <th>alternate-recipient</th>\n",
       "      <th>andorra-islamabad</th>\n",
       "      <th>approved</th>\n",
       "      <th>approved-by</th>\n",
       "      <th>archenemy-matthew</th>\n",
       "      <th>auto-forwarded</th>\n",
       "      <th>auto-submitted</th>\n",
       "      <th>basemen-cankerworm</th>\n",
       "      <th>bcc</th>\n",
       "      <th>...</th>\n",
       "      <th>x-wm-posted-at</th>\n",
       "      <th>x-xam3-api-version</th>\n",
       "      <th>x-yoursite-mailscanner</th>\n",
       "      <th>x-yoursite-mailscanner-information</th>\n",
       "      <th>x-zinester-pid</th>\n",
       "      <th>x_id</th>\n",
       "      <th>x_uid</th>\n",
       "      <th>xmailing-id</th>\n",
       "      <th>xref</th>\n",
       "      <th>isSpam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 289 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  0x-accept-language alternate-recipient andorra-islamabad approved  \\\n",
       "0                NaN                 NaN               NaN      NaN   \n",
       "1                NaN                 NaN               NaN      NaN   \n",
       "2                NaN                 NaN               NaN      NaN   \n",
       "3                NaN                 NaN               NaN      NaN   \n",
       "4                NaN                 NaN               NaN      NaN   \n",
       "\n",
       "  approved-by archenemy-matthew auto-forwarded auto-submitted  \\\n",
       "0         NaN               NaN            NaN            NaN   \n",
       "1         NaN               NaN            NaN            NaN   \n",
       "2         NaN               NaN            NaN            NaN   \n",
       "3         NaN               NaN            NaN            NaN   \n",
       "4         NaN               NaN            NaN            NaN   \n",
       "\n",
       "  basemen-cankerworm  bcc  ... x-wm-posted-at x-xam3-api-version  \\\n",
       "0                NaN  NaN  ...            NaN                NaN   \n",
       "1                NaN  NaN  ...            NaN                NaN   \n",
       "2                NaN  NaN  ...            NaN                NaN   \n",
       "3                NaN  NaN  ...            NaN                NaN   \n",
       "4                NaN  NaN  ...            NaN                NaN   \n",
       "\n",
       "  x-yoursite-mailscanner x-yoursite-mailscanner-information x-zinester-pid  \\\n",
       "0                    NaN                                NaN            NaN   \n",
       "1                    NaN                                NaN            NaN   \n",
       "2                    NaN                                NaN            NaN   \n",
       "3                    NaN                                NaN            NaN   \n",
       "4                    NaN                                NaN            NaN   \n",
       "\n",
       "  x_id x_uid xmailing-id xref isSpam  \n",
       "0  NaN   NaN         NaN  NaN   True  \n",
       "1  NaN   NaN         NaN  NaN   True  \n",
       "2  NaN   NaN         NaN  NaN   True  \n",
       "3  NaN   NaN         NaN  NaN   True  \n",
       "4  NaN   NaN         NaN  NaN   True  \n",
       "\n",
       "[5 rows x 289 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#path = '/home/phrc/Python Project/Spam Email Classifier/emails/ham/beck-s/2001_plan/'\n",
    "#path = '/home/phrc/Python Project/Spam Email Classifier/emails/ham/beck-s/2001_plan/1'\n",
    "path = '/home/phrc/Python Project/Spam Email Classifier/emails/spam/'\n",
    "#path = '/home/phrc/Python Project/Spam Email Classifier/emails/spam/BG/2004/08/1091394468.23940_19.txt'\n",
    "\n",
    "dfEnron2Spam = createDf(path, True)\n",
    "\n",
    "dfEnron2Spam.head()\n",
    "\n",
    "#df = dfEnron2Ham[dfEnron2Ham['fileErro'].notnull()]\n",
    "\n",
    "#df.head()\n",
    "#f = open(path,\"r\")\n",
    "\n",
    "#type(f)\n",
    "\n",
    "#files = [file for file in os.listdir(path) if os.path.isfile(file)]\n",
    "#for file in files:\n",
    "#    print(path+file)\n",
    "#    break\n",
    "\n",
    "#print(os.path.dirname(f))\n",
    "\n",
    "#headers = Parser(policy=default).parsestr(f.read())\n",
    "\n",
    "#print(headers.keys())\n",
    "\n",
    "#print(headers.values())\n",
    "#if headers.get_content_type() == 'text/plain':\n",
    "#    print('OK')\n",
    "#str(headers.get_body())\n",
    "#str(headers.get_content())\n",
    "\n",
    "\n",
    "\n",
    "#t = df.head(1)['text']\n",
    "#type(t[0])\n",
    "#print(t[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "len(dfEnron2Spam)\n",
    "\n",
    "df = dfEnron2Spam[dfEnron2Spam['content-description'].notnull()]\n",
    "print(len(df))\n",
    "\n",
    "#print(df['message'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfEnron2Spam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'parseError'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2655\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2656\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2657\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'parseError'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-8e7c24a88b16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parseError'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#len(dfEnron2Spam)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1911\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1912\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1913\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no slices here, handle elsewhere'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mxs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   3583\u001b[0m                                                       drop_level=drop_level)\n\u001b[1;32m   3584\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3585\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3587\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2657\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2658\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2659\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'parseError'"
     ]
    }
   ],
   "source": [
    "df = dfEnron2Spam.describe(include=['object']).T\n",
    "\n",
    "#df.head(300)\n",
    "#len(df[df['count']>1000])\n",
    "\n",
    "print(len(dfEnron2Spam))\n",
    "#df[df['count']>1000].head(30)\n",
    "\n",
    "\n",
    "df.loc['parseError'].head()\n",
    "#len(dfEnron2Spam)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start spelling \n",
      " corrector total files: 5857 \n",
      "\n",
      " re : telephone interview with enron corp . research dept .\n",
      "dear shirley :\n",
      "confirming that i will be waiting for the telephone interview at 1 pm\n",
      "tomorrow . ? i would like to give you my cell phone number , 713 / 907 - 6717 , as a\n",
      "back - up measure . ? please note that my first preference is to receive the call\n",
      "at my home number , 713 / 669 - 0923 .\n",
      "sincerely ,\n",
      "rabi de\n",
      "?\n",
      "? shirley . crenshaw @ enron . com wrote :\n",
      "dear rabi :\n",
      "i have scheduled the telephone interview for 1 : 00 pm on friday , july 7 th .\n",
      "we will call you at 713 / 669 - 0923 . if there are any changes , please let\n",
      "me know .\n",
      "sincerely ,\n",
      "shirley crenshaw\n",
      "713 - 853 - 5290\n",
      "rabi deon 06 / 26 / 2000 10 : 37 : 24 pm\n",
      "to : shirley crenshaw\n",
      "cc :\n",
      "subject : re : telephone interview with enron corp . research dept .\n",
      "dear ms . crenshaw :\n",
      "thanks for your prompt response . ? july 6 or 7 th will work best for me . . ? i\n",
      "would prefer to be called at my home number . ? please let me know the\n",
      "schedule and other details , if any .\n",
      "sincerely ,\n",
      "rabi de\n",
      "? shirley crenshawwrote :\n",
      "good afternoon mr . de :\n",
      "your resume has been forwarded to the enron corp . re ! search dept . and\n",
      "they would like to conduct a telephone interview with you at your\n",
      "convenience .\n",
      "the interviewers would be :\n",
      "vince kaminski managing director\n",
      "stinson gibner vice president\n",
      "grant masson vice president\n",
      "p . v . krishnarao director\n",
      "paulo issler manager\n",
      "please give me some dates and times this week or july 5 , 6 , and 7 th when\n",
      "you might be available and i will coordinate with the other calendars .\n",
      "i look forward to hearing from you .\n",
      "sincerely ,\n",
      "shirley crenshaw\n",
      "administrative coordinator\n",
      "enron corp . research\n",
      "713 / 853 - 5290\n",
      "email : shirley . crenshaw @ enron . com\n",
      "do you yahoo ! ?\n",
      "get yahoo ! mail - free email you can access from anywhere !\n",
      "do you yahoo ! ?\n",
      "send instant messages & get email alerts with yahoo ! messenger .\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      " re : telephone interview with union corps . research kept .\n",
      "dear shirley :\n",
      "confirming that i will be waiting for the telephone interview at 1 pm\n",
      "tomorrow . ? i would like to give you my cell phone number , 713 / 907 - 6717 , as a\n",
      "back - up measure . ? please note that my first preference is to receive the call\n",
      "at my home number , 713 / 669 - 0923 .\n",
      "sincerely ,\n",
      "rabid de\n",
      "?\n",
      "? shirley . openshaw @ union . com wrote :\n",
      "dear rabid :\n",
      "i have schedule the telephone interview for 1 : 00 pm on friday , july 7 th .\n",
      "we will call you at 713 / 669 - 0923 . if there are any changes , please let\n",
      "me know .\n",
      "sincerely ,\n",
      "shirley openshaw\n",
      "713 - 853 - 5290\n",
      "rabid don 06 / 26 / 2000 10 : 37 : 24 pm\n",
      "to : shirley openshaw\n",
      "c :\n",
      "subject : re : telephone interview with union corps . research kept .\n",
      "dear ms . openshaw :\n",
      "thanks for your prompt response . ? july 6 or 7 th will work best for me . . ? i\n",
      "would prefer to be called at my home number . ? please let me know the\n",
      "schedule and other details , if any .\n",
      "sincerely ,\n",
      "rabid de\n",
      "? shirley crenshawwrote :\n",
      "good afternoon mr . de :\n",
      "your resume has been forwarded to the union corps . re ! search kept . and\n",
      "they would like to conduct a telephone interview with you at your\n",
      "convenience .\n",
      "the interviews would be :\n",
      "since kamenski managing director\n",
      "stanton dinner vice president\n",
      "grant mason vice president\n",
      "p . v . krishnarao director\n",
      "paul sister manager\n",
      "please give me some dates and times this week or july 5 , 6 , and 7 th when\n",
      "you might be available and i will coordinate with the other calendars .\n",
      "i look forward to hearing from you .\n",
      "sincerely ,\n",
      "shirley openshaw\n",
      "administrative coordinate\n",
      "union corps . research\n",
      "713 / 853 - 5290\n",
      "email : shirley . openshaw @ union . com\n",
      "do you yakov ! ?\n",
      "get yakov ! mail - free email you can access from anywhere !\n",
      "do you yakov ! ?\n",
      "send instant messages & get email alert with yakov ! messenger .\n",
      "\n",
      "\n",
      " mscf speaker series\n",
      "mscf speaker series\n",
      "dear mr . kaminsky , i have included the web page of the list of confirmed\n",
      "speakers , most of them are people i worked with as a fixed income bond\n",
      "options trader . ? having you as a speaker would ? give a chance to the mscf\n",
      "students to gain insight in an area ( commodities ) and in a field\n",
      "( research ) ? in which many are ? interested .\n",
      "official invitation\n",
      "?\n",
      "?\n",
      "?\n",
      "the first event is next friday !\n",
      "?\n",
      "first event :\n",
      "august 11 , 2000\n",
      "10 : 30 to 12 : 30 a . m . fast lab\n",
      "david hartney & jerry hanweck\n",
      "vice president , futures and option sales ?\n",
      "j . p . morgan\n",
      "n . b .\n",
      "there will be free caps and a copy of the treasury bond basis . priority will\n",
      "be given to mscf students .\n",
      "?\n",
      "? ?\n",
      "price and hedging volatility contracts\n",
      "september 1 , 2000\n",
      "dmitry pugachevsky\n",
      "deutsche bank\n",
      "dmitry pugachesky is a director with otc derivatives research of deutsche\n",
      "bank , where his research is primarily focussed on credit derivatives . prior\n",
      "to joining deutsche bank , dmitry worked for six years with global analytics\n",
      "group of bankers trust . there he developed models for emerging markets ,\n",
      "interest rates , and equity derivatives and also participated in actual\n",
      "trading and structuring of interest rate options . he received his phd in\n",
      "applied mathematics from carnegie mellon university specializing in control\n",
      "theory for stochastic processes . he has published several papers on\n",
      "modelling in emerging markets and on valuation for passport options .\n",
      "a measurement framework for bank liquidity risk\n",
      "september 15 , 2000\n",
      "raymond cote\n",
      "vice president , finrad inc . nbc\n",
      "raymond cote is vice president , financial engineering at finrad inc . , a\n",
      "montreal - based consulting firm offering financial management solutions that\n",
      "combine advisory and systems development services to & corporations and\n",
      "financial institutions .\n",
      "abstract :\n",
      "liquidity risk , as opposed to credit and market risks , has received little\n",
      "attention in professional or academic journals . we argue that analyzing bank\n",
      "liquidity risk can be viewed as a variation of credit risk analysis . after\n",
      "introducing some concepts and definitions , the presentation defines a\n",
      "framework allowing to measure a bank ' s structural liquidity risk . it then\n",
      "shows that combining the framework with modern credit risk measurement tools\n",
      "leads to a liquidity risk var measure . the presentation then offers\n",
      "concluding comments on the integration of the liquidity risk measurement\n",
      "framework within enterprise - wide risk management .\n",
      "swaps , spreads and bonds\n",
      "september 29 , 2000\n",
      "chris leonard\n",
      "senior trader\n",
      "fixed income arbitrage ?\n",
      "october 27 , 2000\n",
      "chuck mchugh\n",
      "vice president , nbc - new york\n",
      "fund management and market efficiency\n",
      "november 10 , 2000\n",
      "andrea lee\n",
      "portfolio manager , freiss associates\n",
      "pierre - philippe ste - marie\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "http : / / pstemarie . homestead . com\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      " much speaker series\n",
      "much speaker series\n",
      "dear mr . kamensky , i have included the web page of the list of confirmed\n",
      "speakers , most of them are people i worked with as a fixed income bond\n",
      "option trader . ? having you as a speaker would ? give a chance to the much\n",
      "students to gain insight in an area ( commodities ) and in a field\n",
      "( research ) ? in which many are ? interested .\n",
      "official invitation\n",
      "?\n",
      "?\n",
      "?\n",
      "the first event is next friday !\n",
      "?\n",
      "first event :\n",
      "august 11 , 2000\n",
      "10 : 30 to 12 : 30 a . m . fast lab\n",
      "david partner & merry hanweck\n",
      "vice president , future and option sales ?\n",
      "j . p . morgan\n",
      "n . b .\n",
      "there will be free caps and a copy of the treasury bond basis . priority will\n",
      "be given to much students .\n",
      "?\n",
      "? ?\n",
      "price and healing volatility contracts\n",
      "september 1 , 2000\n",
      "dmitri pugachevsky\n",
      "deutsche bank\n",
      "dmitri pugachesky is a director with etc derivatives research of deutsche\n",
      "bank , where his research is primarily focused on credit derivatives . prior\n",
      "to joining deutsche bank , dmitri worked for six years with global analysis\n",
      "group of bankers trust . there he developed models for emerging markets ,\n",
      "interest rates , and equity derivatives and also participated in actual\n",
      "trading and structuring of interest rate option . he received his pad in\n",
      "applied mathematics from carnegie melon university specializing in control\n",
      "theory for stochastic processes . he has published several papers on\n",
      "modeling in emerging markets and on valuation for passport option .\n",
      "a measurement framework for bank liquidity risk\n",
      "september 15 , 2000\n",
      "raymond come\n",
      "vice president , find in . bc\n",
      "raymond come is vice president , financial engineering at find in . , a\n",
      "montreal - based consulting firm offering financial management solutions that\n",
      "combine advisory and systems development services to & corporations and\n",
      "financial institutions .\n",
      "abstract :\n",
      "liquidity risk , as opposed to credit and market risks , has received little\n",
      "attention in professional or academic journals . we argue that analyzing bank\n",
      "liquidity risk can be viewed as a variation of credit risk analysis . after\n",
      "introducing some concepts and definitions , the presentation defines a\n",
      "framework allowing to measure a bank ' s structural liquidity risk . it then\n",
      "shows that combining the framework with modern credit risk measurement tools\n",
      "leads to a liquidity risk war measure . the presentation then offers\n",
      "concluding comments on the integration of the liquidity risk measurement\n",
      "framework within enterprise - wide risk management .\n",
      "swamps , spreads and bonds\n",
      "september 29 , 2000\n",
      "chris leopard\n",
      "senior trader\n",
      "fixed income arbitrate ?\n",
      "october 27 , 2000\n",
      "chuck cough\n",
      "vice president , bc - new york\n",
      "fund management and market efficiency\n",
      "november 10 , 2000\n",
      "andrew lee\n",
      "portfolio manager , fresh associates\n",
      "pierre - philippe she - marie\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "http : / / pstemarie . homestead . com\n",
      "\n",
      "\n",
      " re : info help .\n",
      "krishna ,\n",
      "niclas introduces himself as an associate in the research group .\n",
      "i think we should clarify his status .\n",
      "vince\n",
      "- - - - - - - - - - - - - - - - - - - - - - forwarded by vince j kaminski / hou / ect on 08 / 15 / 2000\n",
      "05 : 53 pm - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "\" michael schilmoeller \" on 08 / 15 / 2000 11 : 08 : 06\n",
      "am\n",
      "to : notes : niclas . egmar @ enron\n",
      "cc : vkamins @ enron . com , grant _ masson @ pgn . com , stinson _ gibner @ pgn . com\n",
      "subject : re : info help .\n",
      "hi niclas ,\n",
      "i am in the middle of preparing some presentations right now , so it might be\n",
      "more productive to speak by phone ( 503 - 464 - 8430 ) . please leave your number ,\n",
      "if you get my voicemail .\n",
      "to get you started , you might see if you can get access to the ferc gads\n",
      "database of plant forced and planned availability . it seems others in\n",
      "research have asked about this , so you may already have this at your\n",
      "disposal . the eia has a good electronic database of plant for and por\n",
      "available for free ( http : / / www . nerc . com / ~ esd / ) . i know alexios in re / ees has\n",
      "this . if you wanted to do it the hard way , you can also ask jaison to access\n",
      "the epa ' s cems data he has summarized on a machine there in research . it\n",
      "contains hourly plant operation for every unit over about 50 mw , which you\n",
      "could aggregate up .\n",
      "the wscc 10 - year forecast of new plant construction and loads is a good place\n",
      "to start for plant construction information , but suffers from some notorious\n",
      "\" self - reporting \" error . it is available in pdf form from the web site\n",
      "http : / / www . wscc . com / . other sources that should be more near - term , but more\n",
      "accurate are the cec inventory of plants ( http : / / www . energy . ca . gov / ) and the\n",
      "bpa whitebook ( http : / / www . transmission . bpa . gov ) .\n",
      "as far as basic economic data is concerned , you can either rely on the\n",
      "reported utility forecasts for loads , or you can go to fundamental data . the\n",
      "ultimate source of the census data collected by the us dept of commerce ,\n",
      "which you can buy on cdrom for cheap . it would have this kind of information\n",
      "by sic code , by zip code . you may also have access to one of the economic\n",
      "forecasting businesses ( wharton ' s wefa , dri , etc . ) they have this in highly\n",
      "digested and complete form .\n",
      "btw , tim heizenrader , who runs fundamental analysis and research on the west\n",
      "desk , is a sharp cookie and should have all this under control . is your\n",
      "client aware of this resource ?\n",
      "give me a buzz and we can talk more ,\n",
      "michael\n",
      "> > > niclas egmar / hou / ees @ enron 08 / 14 / 00 12 : 49 pm > > >\n",
      "michael ,\n",
      "i ' m an analyst in the research group . i would like your help with finding\n",
      "some information specific for the west coast . a new analyst on the west power\n",
      "desk needs information on planned outages and planned new generation . he is\n",
      "studying the long - term fundamentals of electricity volatility on the west\n",
      "coastso so he also needs info on housing starts , computer sales or industrial\n",
      "production figures for computer manufacturing , growth of start - up companies ,\n",
      "and population stats .\n",
      "any help in finding the needed info would be greatly appreciated . contact me\n",
      "or daniel kang ( new analyst ) .\n",
      "niclas\n",
      "- text . htm\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " re : into help .\n",
      "krishna ,\n",
      "nicholas introduces himself as an associate in the research group .\n",
      "i think we should clarify his status .\n",
      "since\n",
      "- - - - - - - - - - - - - - - - - - - - - - forwarded by since j kamenski / you / act on 08 / 15 / 2000\n",
      "05 : 53 pm - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "\" michael schilmoeller \" on 08 / 15 / 2000 11 : 08 : 06\n",
      "am\n",
      "to : notes : nicholas . ear @ union\n",
      "c : vkamins @ union . com , grant _ mason @ pen . com , stanton _ dinner @ pen . com\n",
      "subject : re : into help .\n",
      "hi nicholas ,\n",
      "i am in the middle of preparing some presentations right now , so it might be\n",
      "more productive to speak by phone ( 503 - 464 - 8430 ) . please leave your number ,\n",
      "if you get my voicemail .\n",
      "to get you started , you might see if you can get access to the fera lads\n",
      "database of plant forced and planned availability . it seems others in\n",
      "research have asked about this , so you may already have this at your\n",
      "disposal . the era has a good electronic database of plant for and for\n",
      "available for free ( http : / / www . her . com / ~ end / ) . i know alexis in re / eyes has\n",
      "this . if you wanted to do it the hard way , you can also ask maison to access\n",
      "the pa ' s gems data he has summarized on a machine there in research . it\n",
      "contains hours plant operation for every unit over about 50 my , which you\n",
      "could aggregate up .\n",
      "the sac 10 - year forecast of new plant construction and loads is a good place\n",
      "to start for plant construction information , but suffers from some notorious\n",
      "\" self - reporting \" error . it is available in of form from the web site\n",
      "http : / / www . sac . com / . other sources that should be more near - term , but more\n",
      "accurate are the ce inventor of plants ( http : / / www . energy . ca . go / ) and the\n",
      "pa whitebook ( http : / / www . transmission . pa . go ) .\n",
      "as far as basic economic data is concerned , you can either rely on the\n",
      "reported utility forecasts for loads , or you can go to fundamental data . the\n",
      "ultimate source of the census data collected by the us kept of commerce ,\n",
      "which you can buy on from for cheap . it would have this kind of information\n",
      "by sic code , by zip code . you may also have access to one of the economic\n",
      "forecasting business ( wanton ' s were , dry , etc . ) they have this in highly\n",
      "digested and complete form .\n",
      "bow , tim heizenrader , who runs fundamental analysis and research on the west\n",
      "desk , is a sharp cook and should have all this under control . is your\n",
      "client aware of this resource ?\n",
      "give me a buzz and we can talk more ,\n",
      "michael\n",
      "> > > nicholas ear / you / eyes @ union 08 / 14 / 00 12 : 49 pm > > >\n",
      "michael ,\n",
      "i ' m an analyst in the research group . i would like your help with finding\n",
      "some information specific for the west coast . a new analyst on the west power\n",
      "desk needs information on planned outrages and planned new generation . he is\n",
      "studying the long - term fundamentals of electricity volatility on the west\n",
      "coasts so he also needs into on housing starts , computer sales or industrial\n",
      "production figures for computer manufacturing , growth of start - up companies ,\n",
      "and population states .\n",
      "any help in finding the needed into would be greatly appreciated . contact me\n",
      "or daniel king ( new analyst ) .\n",
      "nicholas\n",
      "- text . htm\n",
      "\n",
      "\n",
      " re : backtesting\n",
      "naveen ,\n",
      "most of these tests have been already coded . the code and the associated\n",
      "spreadsheets may have been lost inn the sands of time . please , check with\n",
      "vasant : it may save you some time .\n",
      "vince\n",
      "naveen andrews @ enron\n",
      "08 / 23 / 2000 05 : 51 pm\n",
      "to : vince j kaminski / hou / ect @ ect\n",
      "cc :\n",
      "subject : backtesting\n",
      "vince ,\n",
      "i am currently implementing the backtesting features into our\n",
      "risk management system . i distributed the following document at our research\n",
      "meeting today ; it outlines the conventional binomial test and some other\n",
      "tests , including the basle regulatory test . of course , no one test is\n",
      "powerful and the efficacy of these tests breakdown for low sample sizes ,\n",
      "etc . if you can think of other tests to complement these , please let me know .\n",
      "regards\n",
      "naveen\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      " re : backtesting\n",
      "haven ,\n",
      "most of these tests have been already code . the code and the associated\n",
      "spreadsheets may have been lost inn the sands of time . please , check with\n",
      "vacant : it may save you some time .\n",
      "since\n",
      "haven andrews @ union\n",
      "08 / 23 / 2000 05 : 51 pm\n",
      "to : since j kamenski / you / act @ act\n",
      "c :\n",
      "subject : backtesting\n",
      "since ,\n",
      "i am currently implementing the backtesting features into our\n",
      "risk management system . i distributed the following document at our research\n",
      "meeting today ; it outlines the conventional binomial test and some other\n",
      "tests , including the basle regulatory test . of course , no one test is\n",
      "powerful and the efficacy of these tests breakdown for low sample sizes ,\n",
      "etc . if you can think of other tests to complement these , please let me know .\n",
      "regards\n",
      "haven\n",
      "\n",
      "\n",
      " powerisk 2000 - more cocktail info\n",
      "- - - - - - - - - - - - - - - - - - - - - - forwarded by iona maclean / lon / ect on 22 / 09 / 2000 12 : 24\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "enron capital & trade resources corp .\n",
      "from : simon turner\n",
      "22 / 09 / 2000 11 : 29\n",
      "to : keiron . ferguson @ accessenergy . nl , mcrosno @ altra . com , ed @ apx . com ,\n",
      "alaw @ avistaenergy . com , markw @ citizenspower . com ,\n",
      "chris _ strickland @ compuserve . com , hbrett @ cyberbuilding . com , geman @ dauphine . fr ,\n",
      "charles . heard @ dc . com , chris . miller @ dc . com , gilbert . toppin @ dc . com ,\n",
      "pat . breen @ dc . com , stuart . beeston @ dc . com , klaus . petzel @ dowjones . com ,\n",
      "sama @ dynegy . com , jdaly @ enermetrix . com , iona . maclean @ enron . com ,\n",
      "vkaminski @ enron . com , mwalsh @ envifi . com , mark @ fea . com ,\n",
      "bachar . samawi @ gen . pge . com , garman @ haas . berkeley . edu ,\n",
      "fgetman @ houstonstreet . com , dave . gardner @ innogy . com , stepheng @ ipe . uk . com ,\n",
      "lecoq _ sophie @ jpmorgan . com , ruckt @ kochind . com , les @ lacima . co . uk ,\n",
      "simon @ localbloke . freeserve . co . uk , carlhans . uhle @ lpx . de , e . westre @ mvv . de ,\n",
      "pwold @ . com , david . whitley @ nordpool . com ,\n",
      "lburke @ nymex . com , xavier . bruckert @ omgroup . com ,\n",
      "aram . sogomonian @ pacificorp . com , peter . haigh @ pgen . com ,\n",
      "sven . otten @ preussenelektra . de , detlef _ r _ hallermann @ reliantenergy . com ,\n",
      "phil . saunders @ southernenergy - europe . nl ,\n",
      "alexander . eydeland @ southernenergy . com , juerg _ trueb @ swissre . com ,\n",
      "alang @ tfs - ln . co . uk , annunziata @ tradecapture . com ,\n",
      "martin . stanley @ txu - europe . com , simon . harrington @ txu - europe . com ,\n",
      "rob @ weatherderivs . com\n",
      "cc :\n",
      "subject : powerisk 2000 - important invitation\n",
      "* * high priority * *\n",
      "dear colleague\n",
      "please find attached two invitations to cocktail parties to be held at\n",
      "powerisk 2000 .\n",
      "you will receive a formal invitation by post . however , just in case this\n",
      "does not reach you by any chance , please print off the attached copies and\n",
      "bring these with you .\n",
      "the cocktails will provide the opportunity to meet all other speakers and\n",
      "delegates .\n",
      "we look forward to seeing you next week .\n",
      "regards\n",
      "rosemary fitzgerald\n",
      "powerisk 2000\n",
      "- cocktail - invite . doc\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-df08e5879d23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#Remove 'Subject:'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Start spelling \\n corrector total files: {} \\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'treatedText'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memailTextCleanner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#Remove Stop Words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3589\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3590\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3591\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3593\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-dec7755bf957>\u001b[0m in \u001b[0;36memailTextCleanner\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"---------------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/textblob/blob.py\u001b[0m in \u001b[0;36mcorrect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr\"\\w+|[^\\w\\s]|\\s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mcorrected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/textblob/blob.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;31m# regex matches: word or punctuation or whitespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr\"\\w+|[^\\w\\s]|\\s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mcorrected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/textblob/blob.py\u001b[0m in \u001b[0;36mcorrect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mversionadded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;36m.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         '''\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspellcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcached_property\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/textblob/blob.py\u001b[0m in \u001b[0;36mspellcheck\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mversionadded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;36m.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         '''\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/textblob/en/__init__.py\u001b[0m in \u001b[0;36msuggest\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \"\"\" Returns a list of (word, confidence)-tuples of spelling corrections.\n\u001b[1;32m    122\u001b[0m     \"\"\"\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mspelling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpolarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/textblob/_text.py\u001b[0m in \u001b[0;36msuggest\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m   1396\u001b[0m         \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                   \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1398\u001b[0;31m                   \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1399\u001b[0m                   \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/textblob/_text.py\u001b[0m in \u001b[0;36m_edit2\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;31m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0;31m# Only keep candidates that are actually known words (20% speedup).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/textblob/_text.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;31m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0;31m# Only keep candidates that are actually known words (20% speedup).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/textblob/_text.py\u001b[0m in \u001b[0;36m__contains__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__iter__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__contains__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__getitem__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/textblob/_text.py\u001b[0m in \u001b[0;36m_lazy\u001b[0;34m(self, method, *args)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethodType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Change this line to your computer path from the dataset\n",
    "path = '/Users/phrc/Documents/Projects/pythonProject/ML assement/enron2/'\n",
    "\n",
    "#Create ham and spam df  \n",
    "dfEnron2Ham = createDf(path+'ham/', False)\n",
    "dfEnron2Spam = createDf(path+'spam/', True)\n",
    "\n",
    "#Append ham and spam df\n",
    "df = dfEnron2Ham.append(dfEnron2Spam)\n",
    "\n",
    "#Remove 'Subject:'  \n",
    "print('Start spelling \\n corrector total files: {} \\n'.format(len(df['text'])))\n",
    "df['treatedText'] = df['text'].apply(emailTextCleanner)\n",
    "\n",
    "#Remove Stop Words\n",
    "stop = text.ENGLISH_STOP_WORDS\n",
    "pat = r'\\b(?:{})\\b'.format('|'.join(stop))\n",
    "df['treatedText'] = df['treatedText'].str.replace(pat, '')\n",
    "\n",
    "\n",
    "#Remove punctuation\n",
    "df['treatedText'] = df['treatedText'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "\n",
    "#Remove extra space and break lines\n",
    "df['treatedText'] = df['treatedText'].str.replace('\\n', ' ')\n",
    "\n",
    "#Create a column with list of words\n",
    "df['wordsList'] = df['treatedText'].str.split().apply(lemmatizeList).apply(revomeWordsWithOneCharacter).apply(removeDigits)\n",
    "\n",
    "\n",
    "\n",
    "#Create a columns to calculate the total amount of words\n",
    "df['totalTreatedWords'] = df['wordsList'].apply(lambda x : len(x))\n",
    "df['treatedTextLen'] = df['treatedText'].apply(lambda x : len(x))\n",
    "df['textLen'] = df['text'].apply(lambda x : len(x))\n",
    "df['uniqueWordsLen'] = df['wordsList'].apply(lambda x : len(set(x)))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset distribution by spam type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['isSpam'].value_counts().rename({False: 'Ham', True: 'Spam'}))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (18,6))\n",
    "grid = plt.GridSpec(1,2, wspace =0.1)\n",
    "barPlt = fig.add_subplot(grid[0,0])\n",
    "piePlt = fig.add_subplot(grid[0,1])\n",
    "\n",
    "barPlt.bar(np.arange(2), df['isSpam'].value_counts(), align='center')\n",
    "barPlt.set_xticks(np.arange(2), ('Ham', 'Spam'))\n",
    "\n",
    "\n",
    "piePlt.pie(df['isSpam'].value_counts(), labels=['Ham', 'Spam'], autopct='%1.0f%%', pctdistance=0.5, labeldistance=1.2)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare text lenght for ham and spam  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 4 differents attributes what define the text lengh:\n",
    "\n",
    "- textLen : This is the raw text without any treatment\n",
    "- treatedTextLen: This is the text after removing the stop words and ponctuation\n",
    "- totalTreatedWords: This is the amount of words used in the text after the treatment, basically this exclude space and line breaks\n",
    "- uniqueWordsLen: This is the amount of unique words used in the treated text, basically it removes repited words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Text size lengh comparison  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (18,6))\n",
    "grid = plt.GridSpec(1,4, wspace =0.8)\n",
    "histPlt = fig.add_subplot(grid[0,0:2])\n",
    "boxPlt1 = fig.add_subplot(grid[0,2])\n",
    "boxPlt2 = fig.add_subplot(grid[0,3])\n",
    "\n",
    "histPlt.hist(\n",
    "    [\n",
    "        df[df['isSpam'] == True]['textLen'], \n",
    "        df[df['isSpam'] == False]['textLen']\n",
    "    ], \n",
    "    np.linspace(0, df['textLen'].quantile(0.75), 30), \n",
    "    density = True, \n",
    "    label=['Spam', 'Ham']\n",
    ") \n",
    "histPlt.legend(loc='upper right')\n",
    "\n",
    "boxPlt1.boxplot(\n",
    "    (\n",
    "         df[df['isSpam'] == True]['textLen'], \n",
    "         df[df['isSpam'] == False]['textLen'] \n",
    "    ), \n",
    "    labels = ('Spam', 'Ham')\n",
    ")\n",
    "\n",
    "boxPlt2.boxplot(\n",
    "    (\n",
    "        df[(df['isSpam'] == True) & (df['textLen'] < df['textLen'].quantile(0.75))]['textLen'], \n",
    "        df[(df['isSpam'] == False) & (df['textLen'] < df['textLen'].quantile(0.75))]['textLen'] \n",
    "    ), \n",
    "    labels = ('Spam', 'Ham')\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text size without stop words and punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (18,6))\n",
    "grid = plt.GridSpec(1,4, wspace =0.8)\n",
    "histPlt = fig.add_subplot(grid[0,0:2])\n",
    "boxPlt1 = fig.add_subplot(grid[0,2])\n",
    "boxPlt2 = fig.add_subplot(grid[0,3])\n",
    "\n",
    "histPlt.hist(\n",
    "    [\n",
    "        df[df['isSpam'] == True]['treatedTextLen'], \n",
    "        df[df['isSpam'] == False]['treatedTextLen']\n",
    "    ],  \n",
    "    np.linspace(0, df['treatedTextLen'].quantile(0.75), 30),\n",
    "    density= True,\n",
    "    label=['Spam', 'Ham']\n",
    ") \n",
    "histPlt.legend(loc='upper right')\n",
    "\n",
    "boxPlt1.boxplot(\n",
    "    (\n",
    "        df[df['isSpam'] == True]['treatedTextLen'], \n",
    "        df[df['isSpam'] == False]['treatedTextLen'] ), \n",
    "    labels = ('Spam', 'Ham')\n",
    ")\n",
    "\n",
    "boxPlt2.boxplot(\n",
    "    (\n",
    "        df[(df['isSpam'] == True) & (df['treatedTextLen'] < df['treatedTextLen'].quantile(0.75))]['treatedTextLen'], \n",
    "        df[(df['isSpam'] == False) & (df['treatedTextLen'] < df['treatedTextLen'].quantile(0.75))]['treatedTextLen'] \n",
    "    ), \n",
    "    labels = ('Spam', 'Ham')\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total words used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (18,6))\n",
    "grid = plt.GridSpec(1,4, wspace =0.8)\n",
    "histPlt = fig.add_subplot(grid[0,0:2])\n",
    "boxPlt1 = fig.add_subplot(grid[0,2])\n",
    "boxPlt2 = fig.add_subplot(grid[0,3])\n",
    "\n",
    "histPlt.hist(\n",
    "    [\n",
    "        df[df['isSpam'] == True]['totalTreatedWords'], \n",
    "        df[df['isSpam'] == False]['totalTreatedWords']\n",
    "    ],  \n",
    "    np.linspace(0, df['totalTreatedWords'].quantile(0.75), 30),\n",
    "    density= True,\n",
    "    label=['Spam', 'Ham']\n",
    ") \n",
    "histPlt.legend(loc='upper right')\n",
    "\n",
    "boxPlt1.boxplot(\n",
    "    (\n",
    "        df[df['isSpam'] == True]['totalTreatedWords'], \n",
    "        df[df['isSpam'] == False]['totalTreatedWords'] \n",
    "    ), \n",
    "    labels = ('Spam', 'Ham')\n",
    ")\n",
    "\n",
    "boxPlt2.boxplot(\n",
    "    (\n",
    "        df[(df['isSpam'] == True) & (df['totalTreatedWords'] < df['totalTreatedWords'].quantile(0.75))]['totalTreatedWords'], \n",
    "        df[(df['isSpam'] == False) & (df['totalTreatedWords'] < df['totalTreatedWords'].quantile(0.75))]['totalTreatedWords'] \n",
    "    ), \n",
    "    labels = ('Spam', 'Ham')\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total unique words used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (18,6))\n",
    "grid = plt.GridSpec(1,4, wspace =0.8)\n",
    "histPlt = fig.add_subplot(grid[0,0:2])\n",
    "boxPlt1 = fig.add_subplot(grid[0,2])\n",
    "boxPlt2 = fig.add_subplot(grid[0,3])\n",
    "\n",
    "histPlt.hist(\n",
    "    [\n",
    "        df[df['isSpam'] == True]['uniqueWordsLen'], \n",
    "        df[df['isSpam'] == False]['uniqueWordsLen']\n",
    "    ],  \n",
    "    np.linspace(0, df['uniqueWordsLen'].quantile(0.75), 30),\n",
    "    density= True,\n",
    "    label=['Spam', 'Ham']\n",
    ") \n",
    "histPlt.legend(loc='upper right')\n",
    "\n",
    "boxPlt1.boxplot(\n",
    "    (\n",
    "        df[df['isSpam'] == True]['uniqueWordsLen'], \n",
    "        df[df['isSpam'] == False]['uniqueWordsLen'] \n",
    "    ), \n",
    "    labels = ('Spam', 'Ham')\n",
    ")\n",
    "\n",
    "boxPlt2.boxplot(\n",
    "    (\n",
    "        df[(df['isSpam'] == True) & (df['uniqueWordsLen'] < df['uniqueWordsLen'].quantile(0.75))]['uniqueWordsLen'], \n",
    "        df[(df['isSpam'] == False) & (df['uniqueWordsLen'] < df['uniqueWordsLen'].quantile(0.75))]['uniqueWordsLen'] \n",
    "    ), \n",
    "    labels = ('Spam', 'Ham')\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "# Need to do \n",
    "\n",
    "----It's not clear the text size can influence into the classification of the email in spam or ham.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total uniques words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter(list(chain(*df[\"wordsList\"].values)))\n",
    "\n",
    "countSpam = Counter(list(chain(*df[df['isSpam'] == True][\"wordsList\"].values)))\n",
    "\n",
    "countHam = Counter(list(chain(*df[df['isSpam'] == False][\"wordsList\"].values)))\n",
    "\n",
    "fig = plt.figure(figsize = (18,6))\n",
    "grid = plt.GridSpec(1,2, wspace =0.2)\n",
    "pltBar = fig.add_subplot(grid[0,0])\n",
    "pltText = fig.add_subplot(grid[0,1])\n",
    "\n",
    "\n",
    "pltBar.bar(\n",
    "    ['Total unique words', 'Spam unique words', 'Ham unique words'], \n",
    "    [len(count), len(countSpam), len(countHam)], \n",
    "    align='center'\n",
    ")\n",
    "\n",
    "textWords = ['Total unique words:              {}'.format(len(count)), \n",
    "             'Total spam unique words:    {}'.format(len(countSpam)), \n",
    "             'Total ham unique words:      {}'.format(len(countHam))]\n",
    "\n",
    "\n",
    "pltText.text(x=0, y=0.5, s = '\\n'.join(textWords), fontsize = 18) \n",
    "pltText.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20 Most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWords = pd.DataFrame(list(dict(count).items()))\n",
    "dfWords.columns = ['word', 'occur'] \n",
    "totalOcurr = dfWords['occur'].sum()\n",
    "dfWords['freq'] = dfWords['occur'] / totalOcurr  \n",
    "dfWords = dfWords.sort_values(by='freq', ascending=False)\n",
    "dfWords = dfWords.reset_index(drop=True)\n",
    "dfWords['freqAcum'] = dfWords['freq'].cumsum()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (18,12))\n",
    "grid = plt.GridSpec(4,2, wspace =0.2, hspace = 0.5)\n",
    "pltLine = fig.add_subplot(grid[0:2,0])\n",
    "pltBar = fig.add_subplot(grid[2:4,0:2])\n",
    "pltTable = fig.add_subplot(grid[0:2,1])\n",
    "\n",
    "pltLine.plot(dfWords['freqAcum'], range(len(dfWords)))\n",
    "pltLine.set_ylabel('Number of Words')\n",
    "pltLine.set_xlabel('Acumulative Frequence')\n",
    "pltLine.set_title('Line Graph of acumulative frequence')\n",
    "pltLine.grid(True)\n",
    "\n",
    "\n",
    "dfTWord = dfWords.loc[[0,20,50,100,500,1000,5000,10000,15000]][['freqAcum']]\n",
    "\n",
    "pltTable.table(cellText=dfTWord.values, rowLabels= dfTWord.index, colLabels = dfTWord.columns, loc='best')\n",
    "pltTable.axis('off')\n",
    "pltTable.set_title('Table of acumulative frequence')\n",
    "\n",
    "\n",
    "\n",
    "pltBar.bar(dfWords.head(20)['word'], dfWords.head(20)['freq'])\n",
    "pltBar.set_title('20 Most Used word')\n",
    "\n",
    "yPos = np.arange(20)\n",
    "pltBar.set_xticklabels(dfWords.head(20)[\"word\"], rotation=60)\n",
    "pltBar.set_xticks(yPos)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20 Most common words on Spam emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSpamWords = pd.DataFrame(list(dict(countSpam).items()))\n",
    "dfSpamWords.columns = ['word', 'occur'] \n",
    "totalSpamOcurr = dfSpamWords['occur'].sum()\n",
    "dfSpamWords['freq'] = dfSpamWords['occur'] / totalSpamOcurr  \n",
    "dfSpamWords = dfSpamWords.sort_values(by='freq', ascending=False)\n",
    "dfSpamWords = dfSpamWords.reset_index(drop=True)\n",
    "dfSpamWords['freqAcum'] = dfSpamWords['freq'].cumsum()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (18,12))\n",
    "grid = plt.GridSpec(4,2, wspace =0.2, hspace = 0.5)\n",
    "pltLine = fig.add_subplot(grid[0:2,0])\n",
    "pltBar = fig.add_subplot(grid[2:4,0:2])\n",
    "pltTable = fig.add_subplot(grid[0:2,1])\n",
    "\n",
    "pltLine.plot(dfSpamWords['freqAcum'], range(len(dfSpamWords)))\n",
    "pltLine.set_ylabel('Number of Words')\n",
    "pltLine.set_xlabel('Acumulative Frequence')\n",
    "pltLine.set_title('Line Graph of acumulative frequence')\n",
    "pltLine.grid(True)\n",
    "\n",
    "\n",
    "dfSWord = dfSpamWords.loc[[0,20,50,100,500,1000,5000,10000,15000]][['freqAcum']]\n",
    "\n",
    "pltTable.table(cellText=dfSWord.values, rowLabels= dfSWord.index, colLabels = dfSWord.columns, loc='best')\n",
    "pltTable.axis('off')\n",
    "pltTable.set_title('Table of acumulative frequence')\n",
    "\n",
    "\n",
    "\n",
    "pltBar.bar(dfSpamWords.head(20)['word'], dfSpamWords.head(20)['freq'], align='center')\n",
    "pltBar.set_title('20 Most Used word')\n",
    "\n",
    "yPos = np.arange(20)\n",
    "pltBar.set_xticklabels(dfSpamWords.head(20)[\"word\"], rotation=60)\n",
    "pltBar.set_xticks(yPos)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20 Most common words on Ham emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHamWords = pd.DataFrame(list(dict(countHam).items()))\n",
    "dfHamWords.columns = ['word', 'occur'] \n",
    "totalHamOcurr = dfHamWords['occur'].sum()\n",
    "dfHamWords['freq'] = dfHamWords['occur'] / totalHamOcurr  \n",
    "dfHamWords = dfHamWords.sort_values(by='freq', ascending=False)\n",
    "dfHamWords = dfHamWords.reset_index(drop=True)\n",
    "dfHamWords['freqAcum'] = dfHamWords['freq'].cumsum()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (18,12))\n",
    "grid = plt.GridSpec(4,2, wspace =0.2, hspace = 0.5)\n",
    "pltLine = fig.add_subplot(grid[0:2,0])\n",
    "pltBar = fig.add_subplot(grid[2:4,0:2])\n",
    "pltTable = fig.add_subplot(grid[0:2,1])\n",
    "\n",
    "pltLine.plot(dfHamWords['freqAcum'], range(len(dfHamWords)))\n",
    "pltLine.set_ylabel('Number of Words')\n",
    "pltLine.set_xlabel('Acumulative Frequence')\n",
    "pltLine.set_title('Line Graph of acumulative frequence')\n",
    "pltLine.grid(True)\n",
    "\n",
    "\n",
    "dfHWord = dfHamWords.loc[[0,20,50,100,500,1000,5000,10000,15000]][['freqAcum']]\n",
    "\n",
    "pltTable.table(cellText=dfHWord.values, rowLabels= dfHWord.index, colLabels = dfHWord.columns, loc='best')\n",
    "pltTable.axis('off')\n",
    "pltTable.set_title('Table of acumulative frequence')\n",
    "\n",
    "\n",
    "\n",
    "pltBar.bar(dfHamWords.head(20)['word'], dfHamWords.head(20)['freq'], align='center')\n",
    "pltBar.set_title('20 Most Used word')\n",
    "\n",
    "yPos = np.arange(20)\n",
    "pltBar.set_xticklabels(dfHamWords.head(20)[\"word\"], rotation=60)\n",
    "pltBar.set_xticks(yPos)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempSet = sorted(set(list(chain(*df[df['isSpam'] == True][\"wordsList\"].values))))\n",
    "\n",
    "\n",
    "print(tempSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "# find those words that may be misspelled\n",
    "misspelled = spell.unknown(tempSet)\n",
    "\n",
    "print(misspelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "words = lem.lemmatize('accommodations')\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting prepare data set for modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(chain(*df[\"wordsList\"].values))\n",
    "\n",
    "l = set(filter(lambda k : len(k) == 1, l))\n",
    "\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWordsG = pd.DataFrame(list(dict(count).items()))\n",
    "dfWordsG.columns = ['word', 'occur'] \n",
    "totalOcurrG = dfWordsG['occur'].sum()\n",
    "dfWordsG['freq'] = dfWordsG['occur'] / totalOcurrG  \n",
    "dfWordsG = dfWordsG.sort_values(by='freq', ascending=False)\n",
    "dfWordsG = dfWordsG.reset_index(drop=True)\n",
    "dfWordsG['freqAcum'] = dfWordsG['freq'].cumsum()\n",
    "\n",
    "dfSpamWordsG = pd.DataFrame(list(dict(countSpam).items()))\n",
    "dfSpamWordsG.columns = ['word', 'occur'] \n",
    "totalSpamOcurrG = dfSpamWordsG['occur'].sum()\n",
    "dfSpamWordsG['freq'] = dfSpamWordsG['occur'] / totalOcurrG  \n",
    "dfSpamWordsG = dfSpamWordsG.sort_values(by='freq', ascending=False)\n",
    "dfSpamWordsG = dfSpamWordsG.reset_index(drop=True)\n",
    "dfSpamWordsG['freqAcum'] = dfSpamWordsG['freq'].cumsum()\n",
    "\n",
    "dfHamWordsG = pd.DataFrame(list(dict(countHam).items()))\n",
    "dfHamWordsG.columns = ['word', 'occur'] \n",
    "totalHamOcurrG = dfHamWordsG['occur'].sum()\n",
    "dfHamWordsG['freq'] = dfHamWordsG['occur'] / totalOcurrG  \n",
    "dfHamWordsG = dfHamWordsG.sort_values(by='freq', ascending=False)\n",
    "dfHamWordsG = dfHamWordsG.reset_index(drop=True)\n",
    "dfHamWordsG['freqAcum'] = dfHamWordsG['freq'].cumsum()\n",
    "\n",
    "fig = plt.figure(figsize = (18,6))\n",
    "grid = plt.GridSpec(1,3, wspace =0.2)\n",
    "pltTotal = fig.add_subplot(grid[0,0])\n",
    "pltSpam = fig.add_subplot(grid[0,1])\n",
    "pltHam = fig.add_subplot(grid[0,2])\n",
    "\n",
    "pltTotal.plot(dfWords['freqAcum'], range(len(dfWordsG)))\n",
    "pltTotal.set_ylabel('Total Words')\n",
    "pltTotal.set_xlabel('Acumulative Frequence')\n",
    "pltTotal.set_title('All Words')\n",
    "pltTotal.grid(True)\n",
    "\n",
    "pltSpam.plot(dfSpamWords['freqAcum'], range(len(dfSpamWordsG)))\n",
    "pltSpam.set_ylabel('Total Spam Words')\n",
    "pltSpam.set_xlabel('Acumulative Frequence')\n",
    "pltSpam.set_title('Spam Words')\n",
    "pltSpam.grid(True)\n",
    "\n",
    "pltHam.plot(dfHamWords['freqAcum'], range(len(dfHamWordsG)))\n",
    "pltHam.set_ylabel('Total Ham Words')\n",
    "pltHam.set_xlabel('Acumulative Frequence')\n",
    "pltHam.set_title('Ham Words')\n",
    "pltHam.grid(True)\n",
    "\n",
    "\n",
    "fig2 = plt.figure(figsize = (18,6))\n",
    "grid2 = plt.GridSpec(1,6, wspace =0.4)\n",
    "\n",
    "pltTotalTableG = fig2.add_subplot(grid2[0,0:2])\n",
    "pltSpamTableG = fig2.add_subplot(grid2[0,2:4])\n",
    "pltHamTableG = fig2.add_subplot(grid2[0,4:6])\n",
    "\n",
    "dfTWordG = dfWordsG.loc[[0,50,100,500,1000,5000,10000,15000]][['freqAcum']]\n",
    "\n",
    "pltTotalTableG.table(cellText=dfTWordG.values, rowLabels= dfTWordG.index, colLabels = dfTWordG.columns, loc='best')\n",
    "pltTotalTableG.axis('off')\n",
    "\n",
    "dfSWordG = dfSpamWordsG.loc[[0,50,100,500,1000,5000,10000,15000]][['freqAcum']]\n",
    "\n",
    "pltSpamTableG.table(cellText=dfSWordG.values, rowLabels= dfSWordG.index, colLabels = dfSWordG.columns, loc='best')\n",
    "pltSpamTableG.axis('off')\n",
    "\n",
    "dfHWordG = dfHamWordsG.loc[[0,50,100,500,1000,5000,10000,15000]][['freqAcum']]\n",
    "\n",
    "pltHamTableG.table(cellText=dfHWordG.values, rowLabels= dfHWordG.index, colLabels = dfHWordG.columns, loc='best')\n",
    "pltHamTableG.axis('off')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
